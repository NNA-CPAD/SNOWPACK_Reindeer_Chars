{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28a2b431-99f0-44e3-bd33-5d1a56b9c629",
   "metadata": {},
   "source": [
    "# File Convert Raw\n",
    "**Description:** Function for Extracting Relevant Data from Raw .pro File and Converting to .csv File\n",
    "**Input Data:** Ascii file of .pro (snowpack profile) output from SNOWPACK model  \n",
    "**Output Data:** csv file of snow depth, max density, water content at max density, season, max ice volume, and max density index\n",
    "**Creator:** Emma Perkins  \n",
    "**Date:** November 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbb54e81-8cdb-4a53-a5ee-5675be05c914",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.colors as colors\n",
    "import datetime\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698d57a4-d36f-4399-b2a0-95bb3eddbacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_convert_raw(folder, model_run):\n",
    "    #define supporting functions-------------------------------------------------------------------------------\n",
    "    def extract_all(result):\n",
    "        first_data_idx = result.index[result['[STATION_PARAMETERS]'] == '[DATA]'][0] + 1\n",
    "\n",
    "        #save header separately from data\n",
    "        header = result['[STATION_PARAMETERS]'].iloc[0:first_data_idx-1]\n",
    "        data = result['[STATION_PARAMETERS]'].iloc[first_data_idx:-2]\n",
    "\n",
    "        #find the rows with date\n",
    "        date = pd.to_datetime(data.loc[data.str.startswith('0500', na=False)].str.slice(5), format='%d.%m.%Y %H:%M:%S').to_numpy()\n",
    "\n",
    "        #find rows with depth\n",
    "        depth = data.loc[data.str.startswith('0501', na=False)].str.slice(5).str.rsplit(',').str[-1].astype(float).to_numpy()\n",
    "\n",
    "        #find indices of date, depth, density, water content, and ice volume\n",
    "        date_idx = data.loc[data.str.startswith('0500', na=False)].index.values\n",
    "        depth_idx = data.loc[data.str.startswith('0501', na=False)].index.values\n",
    "        den_idx = data.loc[data.str.startswith('0502', na=False)].index.values\n",
    "        wc_idx = data.loc[data.str.startswith('0506', na=False)].index.values\n",
    "        ice_vol_idx = data.loc[data.str.startswith('0515', na=False)].index.values\n",
    "\n",
    "        #create dataframe with date and depth\n",
    "        full_data = pd.DataFrame({'date': date, 'depth': depth})\n",
    "\n",
    "        #initialize columns of max_den and WC_dmax\n",
    "        full_data['max_den'] = np.nan \n",
    "        full_data['WC_dmax'] = np.nan\n",
    "        full_data['max_ice_vol'] = np.nan\n",
    "        full_data['den_max_idx'] = np.nan\n",
    "\n",
    "        #populate with max densities and ice volumes for dates with depth greater than 0\n",
    "        sorter = np.argsort(date_idx)\n",
    "        full_date_pd_ids = sorter[np.searchsorted(date_idx, den_idx - 2, sorter=sorter)]\n",
    "        full_data.loc[full_date_pd_ids, 'max_den'] = [np.array(i[1:]).astype(float).max() for i in data[den_idx].str.slice(5).str.rsplit(',')]\n",
    "        full_data.loc[full_date_pd_ids, 'max_ice_vol'] = [np.array(i[1:]).astype(float).max() for i in data[ice_vol_idx].str.slice(5).str.rsplit(',')]\n",
    "\n",
    "        #find water content corresponding to layer of maximum density at each timestep------------------------------------------------------------------\n",
    "        #find index of max density layer for each timestep\n",
    "        full_data.loc[full_date_pd_ids, 'den_max_idx'] = [np.array(i[1:]).astype(float).argmax() for i in data[den_idx].str.slice(5).str.rsplit(',')]\n",
    "\n",
    "        #find dates that have full data (depth greater than 0 aka max density has a value, not na)\n",
    "        full_dates = full_data[~full_data['den_max_idx'].isna()]\n",
    "\n",
    "        #get index (integer) of maximum density for dates that have full data\n",
    "        full_dates['den_max_idx'] = full_dates.den_max_idx.astype(int)\n",
    "\n",
    "        #calculate water content at index of max density\n",
    "        WCs = np.array([])\n",
    "        d = 0\n",
    "        den_max_ids = full_dates.den_max_idx.reset_index().den_max_idx\n",
    "        for i in data[wc_idx].str.slice(5).str.rsplit(','):\n",
    "            WCs = np.append(WCs,float(np.array(i[den_max_ids[d]+1])))\n",
    "            d = d + 1\n",
    "        full_dates['WC_dmax'] = WCs\n",
    "\n",
    "        #populate full_data dataframe with full_dates data\n",
    "        full_data[~full_data['den_max_idx'].isna()] = full_dates\n",
    "\n",
    "        #drop den_max_idx since we no longer need it\n",
    "        #full_data = full_data.drop('den_max_idx')\n",
    "\n",
    "        return full_data\n",
    "    \n",
    "    #supporting function for assigning season to dates\n",
    "    def assign_season(row):\n",
    "        year = row.date.year\n",
    "        if datetime.datetime(year, 7, 15) < row.date < datetime.datetime(year + 1, 7, 15):\n",
    "            season = year\n",
    "        else:\n",
    "            season = year - 1\n",
    "        return season\n",
    "    \n",
    "    #END OF SUPPORTING FUNCTIONS-----------------------------------------------------------------------------------------------------------------------\n",
    "    # Get data file names\n",
    "    main_path = '/glade/work/eperkins/SNOWPACK_data/Results/pro_files/GE_alts/'+folder+'/'+model_run+'/' #change to appropriate path\n",
    "    filenames = sorted(glob.glob(main_path + \"*.pro\"))\n",
    "    \n",
    "    #read data for all files\n",
    "    results = []\n",
    "    for filename in filenames:\n",
    "        results.append(pd.read_csv(filename, sep = '\\n')) #read each file into separate pandas dataframe\n",
    "        \n",
    "    #extract data for all points\n",
    "    full_data = list(range(len(filenames)))\n",
    "    for i in range(len(filenames)):\n",
    "        full_data[i] = extract_all(results[i])\n",
    "        \n",
    "    #assign seasons\n",
    "    for i in range(len(filenames)):\n",
    "        full_data[i]['season'] = full_data[i].apply(assign_season, axis = 1)\n",
    "        full_data[i].to_csv('/glade/work/eperkins/SNOWPACK_data/Results/analysis/GE_alts/csv_files/'+folder+'/'+model_run+'/'+'Point'+str(i+1)+'.csv')\n",
    "    \n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732ca37d-06dd-425d-8cd1-52be90ff5d75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/eperkins/miniconda3/envs/analysis3/lib/python3.7/site-packages/ipykernel_launcher.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/glade/work/eperkins/miniconda3/envs/analysis3/lib/python3.7/site-packages/ipykernel_launcher.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12min 38s, sys: 30.2 s, total: 13min 9s\n",
      "Wall time: 13min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "folder = '1950_2014'\n",
    "full_runs = ['LE1','LE2','LE3','LE4','LE5','LE6','LE7','LE8','LE9','LE10']\n",
    "for model_run in full_runs:\n",
    "    full_data = file_convert_raw(folder, model_run)\n",
    "    #print(full_data[0].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-analysis3]",
   "language": "python",
   "name": "conda-env-miniconda3-analysis3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
