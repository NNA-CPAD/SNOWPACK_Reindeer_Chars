{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14bc700c-747f-4484-b87a-a1a92af90d1f",
   "metadata": {},
   "source": [
    "# Script for Preparing NetCDF Data for SNOWPACK - Multiyear CESM2LE\n",
    "**Description:** Converts NetCDF data to SMET type and imposes user-defined precip cutoff    \n",
    "**Input Data:** TSA_R, RH2M_R, UBOT, VBOT (all 3-hourly), FSDS, FLDS, TS (all daily), PRECT (hourly)  \n",
    "**Output Data:** SMET file for SNOWPACK  \n",
    "**Author:** Emma Perkins  \n",
    "**Date:** June 2023  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cf87150-325c-4623-a812-40f9b49f3024",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in relevant packages\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import pandas as pd\n",
    "import math\n",
    "import re\n",
    "import calendar\n",
    "import cftime\n",
    "import glob\n",
    "#!pip install pysolar\n",
    "from pysolar.solar import *\n",
    "import time\n",
    "import os\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d54d597b-03b9-46cd-9524-b714ca147ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for dask optimization\n",
    "nworkers = 30\n",
    "chunk_time = 200\n",
    "chunk_lat = 100\n",
    "chunk_lon = 100\n",
    "nmem='20'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "580c810e-4906-4da5-afc9-674b52f4c257",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/glade/work/eperkins/miniconda3/envs/analysis3/lib/python3.7/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
      "Perhaps you already have a cluster running?\n",
      "Hosting the HTTP server on port 45667 instead\n",
      "  f\"Port {expected} is already in use.\\n\"\n"
     ]
    }
   ],
   "source": [
    "# Import dask\n",
    "import dask\n",
    "\n",
    "# Use dask jobqueue\n",
    "from dask_jobqueue import PBSCluster\n",
    "\n",
    "# Import a client\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Setup your PBSCluster\n",
    "cluster = PBSCluster(\n",
    "    cores=1, # The number of cores you want\n",
    "    memory=nmem+'GiB', # Amount of memory\n",
    "    processes=1, # How many processes\n",
    "    queue='casper', # The type of queue to utilize (/glade/u/apps/dav/opt/usr/bin/execcasper)\n",
    "    local_directory='/glade/scratch/$USER/local_dask', # Use your local directory\n",
    "    resource_spec='select=1:ncpus=1:mem='+nmem+'GB', # Specify resources\n",
    "    project='P93300065', # Input your project ID here, previously this was known as 'project', now is 'account'\n",
    "    walltime='04:00:00', # Amount of wall time\n",
    "    #interface=\"ib0\" # Interface to use\n",
    ")\n",
    "\n",
    "# Scale up\n",
    "cluster.scale(nworkers)\n",
    "\n",
    "# Change your url to the dask dashboard so you can see it\n",
    "dask.config.set({'distributed.dashboard.link':'https://jupyterhub.hpc.ucar.edu/stable/user/{USER}/proxy/{port}/status'})\n",
    "\n",
    "# Setup your client\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96964e2d-e56d-41cb-94c9-05a6e4e173a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\"> </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px;\">Client</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Client-9ee383d8-7f2a-11ee-83c9-3cecef1b11de</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "\n",
       "        <tr>\n",
       "        \n",
       "            <td style=\"text-align: left;\"><strong>Connection method:</strong> Cluster object</td>\n",
       "            <td style=\"text-align: left;\"><strong>Cluster type:</strong> dask_jobqueue.PBSCluster</td>\n",
       "        \n",
       "        </tr>\n",
       "\n",
       "        \n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard: </strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/eperkins/proxy/45667/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/eperkins/proxy/45667/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\"></td>\n",
       "            </tr>\n",
       "        \n",
       "\n",
       "        </table>\n",
       "\n",
       "        \n",
       "            <details>\n",
       "            <summary style=\"margin-bottom: 20px;\"><h3 style=\"display: inline;\">Cluster Info</h3></summary>\n",
       "            <div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-mod-trusted jp-OutputArea-output\">\n",
       "    <div style=\"width: 24px; height: 24px; background-color: #e1e1e1; border: 3px solid #9D9D9D; border-radius: 5px; position: absolute;\">\n",
       "    </div>\n",
       "    <div style=\"margin-left: 48px;\">\n",
       "        <h3 style=\"margin-bottom: 0px; margin-top: 0px;\">PBSCluster</h3>\n",
       "        <p style=\"color: #9D9D9D; margin-bottom: 0px;\">5cafdb4c</p>\n",
       "        <table style=\"width: 100%; text-align: left;\">\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/eperkins/proxy/45667/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/eperkins/proxy/45667/status</a>\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Workers:</strong> 0\n",
       "                </td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total threads:</strong> 0\n",
       "                </td>\n",
       "                <td style=\"text-align: left;\">\n",
       "                    <strong>Total memory:</strong> 0 B\n",
       "                </td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "\n",
       "        <details>\n",
       "            <summary style=\"margin-bottom: 20px;\">\n",
       "                <h3 style=\"display: inline;\">Scheduler Info</h3>\n",
       "            </summary>\n",
       "\n",
       "            <div style=\"\">\n",
       "    <div>\n",
       "        <div style=\"width: 24px; height: 24px; background-color: #FFF7E5; border: 3px solid #FF6132; border-radius: 5px; position: absolute;\"> </div>\n",
       "        <div style=\"margin-left: 48px;\">\n",
       "            <h3 style=\"margin-bottom: 0px;\">Scheduler</h3>\n",
       "            <p style=\"color: #9D9D9D; margin-bottom: 0px;\">Scheduler-d41a4b79-bd0c-4df2-99b6-7b4cf7d48659</p>\n",
       "            <table style=\"width: 100%; text-align: left;\">\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Comm:</strong> tcp://128.117.208.100:42547\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Workers:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Dashboard:</strong> <a href=\"https://jupyterhub.hpc.ucar.edu/stable/user/eperkins/proxy/45667/status\" target=\"_blank\">https://jupyterhub.hpc.ucar.edu/stable/user/eperkins/proxy/45667/status</a>\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total threads:</strong> 0\n",
       "                    </td>\n",
       "                </tr>\n",
       "                <tr>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Started:</strong> Just now\n",
       "                    </td>\n",
       "                    <td style=\"text-align: left;\">\n",
       "                        <strong>Total memory:</strong> 0 B\n",
       "                    </td>\n",
       "                </tr>\n",
       "            </table>\n",
       "        </div>\n",
       "    </div>\n",
       "\n",
       "    <details style=\"margin-left: 48px;\">\n",
       "        <summary style=\"margin-bottom: 20px;\">\n",
       "            <h3 style=\"display: inline;\">Workers</h3>\n",
       "        </summary>\n",
       "\n",
       "        \n",
       "\n",
       "    </details>\n",
       "</div>\n",
       "\n",
       "        </details>\n",
       "    </div>\n",
       "</div>\n",
       "            </details>\n",
       "        \n",
       "\n",
       "    </div>\n",
       "</div>"
      ],
      "text/plain": [
       "<Client: 'tcp://128.117.208.100:42547' processes=0 threads=0, memory=0 B>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client  #wait a cuople mins before running this cell to give workers some time to spin up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60956d3b-d30d-40b0-ba88-482b1cf3fef5",
   "metadata": {},
   "source": [
    "## **Supporting Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29119875-0495-4f9a-8593-907327e8916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(ensemble_member):\n",
    "    #function to read in data for all SNOWPACK required variables for a given ensemble member\n",
    "    LE_pairs = [['01','01'], ['02','03'],['03','05'],['04','07'],['05','09'],['06','11'],['07','13'],['08','15'],['09','17'],['10','19']]\n",
    "    first = LE_pairs[ensemble_member-1][1]\n",
    "    second = LE_pairs[ensemble_member-1][0]\n",
    "\n",
    "    #master paths to directories for different types of variables\n",
    "    lnd_path_3 = '/glade/campaign/cgd/cesm/CESM2-LE/lnd/proc/tseries/hour_3/'\n",
    "    lnd_path_day = '/glade/campaign/cgd/cesm/CESM2-LE/lnd/proc/tseries/day_1/'\n",
    "    atm_path_3 = '/glade/campaign/cgd/cesm/CESM2-LE/atm/proc/tseries/hour_3/'\n",
    "    atm_path_day = '/glade/campaign/cgd/cesm/CESM2-LE/atm/proc/tseries/day_1/'\n",
    "\n",
    "    #air temp:\n",
    "    at_names = 'TSA_R/b.e21.BSSP370smbb.f09_g17.LE2-1'+first+'1.0'+second+'.clm2.h7.TSA_R.*.nc' #rural 2 meter air temp (3-hourly)\n",
    "    at_files = sorted(glob.glob(lnd_path_3 + at_names))\n",
    "    at_data = xr.open_mfdataset(at_files, parallel = True, chunks={\"time\":chunk_time, \"lat\":chunk_lat, \"lon\":chunk_lon})\n",
    "\n",
    "    #relative humidity:\n",
    "    rh_names = 'RH2M_R/b.e21.BSSP370smbb.f09_g17.LE2-1'+first+'1.0'+second+'.clm2.h7.RH2M_R.*.nc' #rural 2 meter air temp (3-hourly)\n",
    "    rh_files = sorted(glob.glob(lnd_path_3 + rh_names))\n",
    "    rh_data = xr.open_mfdataset(rh_files, parallel = True, chunks={\"time\":chunk_time, \"lat\":chunk_lat, \"lon\":chunk_lon})\n",
    "\n",
    "    #u wind:\n",
    "    u_names = 'UBOT/b.e21.BSSP370smbb.f09_g17.LE2-1'+first+'1.0'+second+'.cam.h3.UBOT.*.nc' #bottom level u wind (3- hourly)\n",
    "    u_files = sorted(glob.glob(atm_path_3 + u_names))\n",
    "    u_data = xr.open_mfdataset(u_files, parallel = True, chunks={\"time\":chunk_time, \"lat\":chunk_lat, \"lon\":chunk_lon})\n",
    "\n",
    "    #v wind:\n",
    "    v_names = 'VBOT/b.e21.BSSP370smbb.f09_g17.LE2-1'+first+'1.0'+second+'.cam.h3.VBOT.*.nc' #bottom level v wind (3-hourly)\n",
    "    v_files = sorted(glob.glob(atm_path_3 + v_names))\n",
    "    v_data = xr.open_mfdataset(v_files, parallel = True, chunks={\"time\":chunk_time, \"lat\":chunk_lat, \"lon\":chunk_lon})\n",
    "\n",
    "    #incoming shortwave radiation:\n",
    "    sw_names = 'FSDS/b.e21.BSSP370smbb.f09_g17.LE2-1'+first+'1.0'+second+'.cam.h1.FSDS.*.nc' #incoming solar radiation at surface (daily)\n",
    "    sw_files = sorted(glob.glob(atm_path_day + sw_names))\n",
    "    sw_data = xr.open_mfdataset(sw_files, parallel = True, chunks={\"time\":chunk_time, \"lat\":chunk_lat, \"lon\":chunk_lon})\n",
    "\n",
    "    #incoming longwave radiation:\n",
    "    lw_names = 'FLDS/b.e21.BSSP370smbb.f09_g17.LE2-1'+first+'1.0'+second+'.cam.h1.FLDS.*.nc' #incoming solar radiation at surface (daily)\n",
    "    lw_files = sorted(glob.glob(atm_path_day + lw_names))\n",
    "    lw_data = xr.open_mfdataset(lw_files, parallel = True, chunks={\"time\":chunk_time, \"lat\":chunk_lat, \"lon\":chunk_lon})\n",
    "\n",
    "    #total precipitation\n",
    "    prcp_path = '/glade/campaign/cgd/cesm/CESM2-LE/atm/proc/tseries/hour_1/PRECT/'\n",
    "    prcp_names = 'b.e21.BSSP370smbb.f09_g17.LE2-1'+first+'1.0'+second+'.cam.h4.PRECT.*.nc' #hourly\n",
    "    prcp_files = sorted(glob.glob(prcp_path + prcp_names))\n",
    "    prcp_data = xr.open_mfdataset(prcp_files, parallel = True, chunks={\"time\":chunk_time, \"lat\":chunk_lat, \"lon\":chunk_lon})\n",
    "\n",
    "    #ground temp\n",
    "    gt_names = 'TG/b.e21.BSSP370smbb.f09_g17.LE2-1031.002.clm2.h5.TG.*.nc'\n",
    "    gt_files = sorted(glob.glob(lnd_path_day + gt_names))\n",
    "    gt_data = xr.open_mfdataset(gt_files, parallel = True, chunks={\"time\":chunk_time, \"lat\":chunk_lat, \"lon\":chunk_lon})\n",
    "\n",
    "    #surface temp\n",
    "    st_names = 'TSKIN/b.e21.BSSP370smbb.f09_g17.LE2-1031.002.clm2.h5.TSKIN.*.nc'\n",
    "    st_files = sorted(glob.glob(lnd_path_day + st_names))\n",
    "    st_data = xr.open_mfdataset(st_files, parallel = True, chunks={\"time\":chunk_time, \"lat\":chunk_lat, \"lon\":chunk_lon})\n",
    "    \n",
    "    return (at_data, rh_data, u_data, v_data, sw_data, lw_data, gt_data, st_data, prcp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35373660-f62f-4cff-a14a-24e4d4b1d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(at_data, rh_data, u_data, v_data, sw_data, lw_data, gt_data, st_data, prcp_data, lat_coord, lon_coord, precip_cutoff, start, end):\n",
    "    #function for preparing data read in with read_data() to SNOWPACK specifications\n",
    "    \n",
    "    #Unit/Data Type notes:\n",
    "    #lat_coord: float\n",
    "    #lon_coord: float\n",
    "    #precip_cutoff: foat (in mm)\n",
    "    #start: string of start date in format 'YYYY-MM-DD'\n",
    "    #end: string of end date in format 'YYYY-MM-DD'\n",
    "    \n",
    "    #1. Select only variables of interest - reduces computational cost--------------------------------------------------------------------------------------------------------------\n",
    "    all_data = [at_data.TSA_R, rh_data.RH2M_R, u_data.UBOT, v_data.VBOT, sw_data.FSDS, lw_data.FLDS, prcp_data.PRECT, gt_data.TG, st_data.TSKIN] #make list of all dataframes and select only variables of interest\n",
    "    #all_data = [at_data, rh_data, u_data, v_data, sw_data, lw_data, prcp_data, gt_data]\n",
    "    \n",
    "    #2. Select only data for location and timeframe of interest and interpolate to hourly resolution--------------------------------------------------------------------------------\n",
    "    loc_data = list(range(len(all_data))) #initialize list of point-specific data\n",
    "    for i in range(len(all_data)):\n",
    "        loc_data[i] = all_data[i].sel(lat = lat_coord, method = 'nearest').sel(lon = lon_coord, method = 'nearest').resample(time='1H').interpolate('cubic').sel(time=slice(start, end))\n",
    "        loc_data[i]['lat'] = round(float(loc_data[i].lat.values),2) # round all latitudes to 2 decimal places so that they match exactly for data merging\n",
    "    \n",
    "    #3. Merge all remaining data into one array and rename variables to match SNOWPACK inputs---------------------------------------------------------------------------------------\n",
    "    full_data = xr.merge(loc_data)\n",
    "    full_data = full_data.rename({'TSA_R': 'TA', 'RH2M_R': 'RH', 'TG': 'TSG', 'TSKIN': 'TSS', 'FSDS' : 'ISWR', 'FLDS': 'ILWR', 'UBOT': 'U', 'VBOT': 'V'})\n",
    "    #print('Done with point selection')\n",
    "\n",
    "    #4. Calculate clear sky ISWR-----------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "    solar_alt = get_altitude_fast(lat_coord, lon_coord, full_data.indexes['time'].to_datetimeindex().tz_localize('UTC')) #create numpy of solar altitude for each time step\n",
    "    data_solar = xr.DataArray(solar_alt, coords = {'lon': full_data.lon.values, 'lat': full_data.lat.values, 'time':  full_data.indexes['time']}, \n",
    "                              dims = ['time'], name = 'solar_alt') #convert numpy array to xarray for merging with full_data\n",
    "    full_data = xr.merge([full_data, data_solar])\n",
    "    full_data = full_data.assign(ISWR_clear = lambda df: radiation.get_radiation_direct(df.time, df.solar_alt)) #calculate clear sky radiation from solar altitude\n",
    "    full_data['ISWR_clear'] = full_data.ISWR_clear.where(full_data.solar_alt >= 0, other = 0) #articficially make ISWR 0 when solar altitude is below horizon since pysolar will not automatically do this\n",
    "\n",
    "    #5. correct clear sky ISWR according to CESM2 daily mean------------------------------------------------------------------------------------------------------------------------\n",
    "    #create arrays with the true and clear mean of each day repeated 24 times (once per hour) to concatenate with hourly xarray dataframe\n",
    "    ISWR_true_mean_repeat = np.repeat(sw_data.FSDS.sel(lat = lat_coord, method = 'nearest').sel(lon = lon_coord, method = 'nearest').sel(time=slice(start, end)).values, 24)\n",
    "    ISWR_clear_mean_repeat = np.repeat(full_data.ISWR_clear.resample(time='1D').mean(dim='time').values, 24)\n",
    "\n",
    "    #turn mean numpy arrays into xarray format\n",
    "    ISWR_data_clear = xr.DataArray(ISWR_clear_mean_repeat, coords = {'lon': full_data.lon.values, 'lat': full_data.lat.values, 'time':  full_data.indexes['time']}, \n",
    "                              dims = ['time'], name = 'ISWR_clear_mean')\n",
    "    ISWR_data_true = xr.DataArray(ISWR_true_mean_repeat, coords = {'lon': full_data.lon.values, 'lat': full_data.lat.values, 'time':  full_data.indexes['time']}, \n",
    "                              dims = ['time'], name = 'ISWR_true_mean')\n",
    "\n",
    "    #merge means with rest of the data\n",
    "    full_data = xr.merge([full_data, ISWR_data_clear, ISWR_data_true])\n",
    "\n",
    "    #calculate corrected ISWR using ISWR_clear and both means\n",
    "    full_data = full_data.assign(ISWR_corrected = lambda df: df.ISWR_clear - df. ISWR_clear) #create variable of all zeros\n",
    "    full_data['ISWR_corrected'] = full_data.ISWR_corrected.where(full_data.ISWR_clear_mean == 0, other = full_data.ISWR_clear*full_data.ISWR_true_mean/full_data.ISWR_clear_mean)  \n",
    "        #in places where clear mean isn't zero, adjust ISWR_clear by true_mean/clear_mean\n",
    "        #leave ISWR_corrected as 0 in places where clear_mean is zero because that means there is no incident radiation and dividing by zero gives nans\n",
    "    \n",
    "    full_data = full_data.drop('ISWR')\n",
    "    full_data = full_data.rename({'ISWR_corrected': 'ISWR'})\n",
    "    #print('Done with ISWR correction')\n",
    "    \n",
    "    \n",
    "    #6. Calculate clear sky ILWR according to Idso and Jackson (1969)\n",
    "    full_data = full_data.assign(ILWR_clear = lambda df: 5.67*10**(-8)*(df.TA**4)*(1-0.261*np.exp(-7.77*10**(-4)*(273-df.TA)**2)))\n",
    "\n",
    "    #7. Correct clear sky ILWR according to CESM2 daily mean\n",
    "    #create arrays with the true and clear mean of each day repeated 24 times (once per hour) to concatenate with hourly xarray dataframe\n",
    "    ILWR_true_mean_repeat = np.repeat(lw_data.FLDS.sel(lat = lat_coord, method = 'nearest').sel(lon = lon_coord, method = 'nearest').sel(time=slice(start, end)).values, 24)\n",
    "    ILWR_clear_mean_repeat = np.repeat(full_data.ILWR_clear.resample(time='1D').mean(dim='time').values, 24)\n",
    "\n",
    "    #turn mean numpy arrays into xarray format\n",
    "    ILWR_data_clear = xr.DataArray(ILWR_clear_mean_repeat, coords = {'lon': full_data.lon.values, 'lat': full_data.lat.values, 'time':  full_data.indexes['time']}, \n",
    "                              dims = ['time'], name = 'ILWR_clear_mean')\n",
    "    ILWR_data_true = xr.DataArray(ILWR_true_mean_repeat, coords = {'lon': full_data.lon.values, 'lat': full_data.lat.values, 'time':  full_data.indexes['time']}, \n",
    "                              dims = ['time'], name = 'ILWR_true_mean')\n",
    "\n",
    "    #merge means with rest of the data\n",
    "    full_data = xr.merge([full_data, ILWR_data_clear, ILWR_data_true])\n",
    "\n",
    "    #calculate corrected ISWR using ISWR_clear and both means\n",
    "    full_data = full_data.assign(ILWR_corrected = lambda df: df.ILWR_clear - df. ILWR_clear) #create variable of all zeros\n",
    "    full_data['ILWR_corrected'] = full_data.ILWR_corrected.where(full_data.ILWR_clear_mean == 0, other = full_data.ILWR_clear*full_data.ILWR_true_mean/full_data.ILWR_clear_mean)\n",
    "        #in places where clear mean isn't zero, adjust ISLR_clear by true_mean/clear_mean\n",
    "        #leave ILWR_corrected as 0 in places where clear_mean is zero because that means there is no incident radiation and dividing by zero gives nans\n",
    "        \n",
    "    full_data = full_data.drop('ILWR')\n",
    "    full_data = full_data.rename({'ILWR_corrected': 'ILWR'})\n",
    "    #print('Done with ILWR correction')\n",
    "    \n",
    "    #6. Convert wind components to total velocity (VW) and direction (DW)-----------------------------------------------------------------------------------------------------------\n",
    "    full_data['VW'] = ((full_data.U)**2 + (full_data.V)**2)**(1/2)\n",
    "    full_data['DW'] = np.arctan(full_data.V / full_data.U)\n",
    "    #print('Done with wind components conversion')\n",
    "\n",
    "    #7. convert precipitation and impose cutoff\n",
    "    full_data['PSUM'] = full_data.PRECT * 3.6 * 10**6 #convert m/s to cummulative precip (mm)\n",
    "    full_data['PSUM'] = full_data.PSUM.where(full_data.PSUM >= precip_cutoff, other = 0)\n",
    "    #print('Done with precip cutoff')\n",
    "\n",
    "    #8. convert RH from percent to decimal\n",
    "    full_data['RH'] = full_data.RH / 100\n",
    "\n",
    "    #9. drop all intermediate variables\n",
    "    data4analysis = full_data[['TA', 'RH', 'TSG', 'TSS', 'PSUM', 'VW', 'DW', 'ILWR', 'ISWR']]\n",
    "    #data4analysis = full_data.drop(['true_mean', 'clear_mean', 'solar_alt', 'VBOT', 'UBOT', 'ISWR_clear', 'PRECT'])    \n",
    "    \n",
    "    #10. account for leap years\n",
    "    times = data4analysis.indexes['time'].to_datetimeindex()\n",
    "    new = np.empty(times.shape, dtype=\"O\")\n",
    "\n",
    "    for i, t in enumerate(times):\n",
    "        dt = cftime.DatetimeAllLeap(t.year, t.month, t.day, t.hour, t.minute, t.second, t.microsecond) #convert calendar type to 'all_leap' from 'no_leap'\n",
    "        new[i] = dt\n",
    "        \n",
    "    data4analysis['time'] = new #assign new calendar type (still has exact same dates)\n",
    "    \n",
    "    datetimeleap = xr.cftime_range(start = start, end = end[:-1]+str(int(end[-1])+1), \n",
    "                    calendar = 'all_leap', freq='H') #create new calendar where every year has a leap day\n",
    "    datetimeleap = datetimeleap[:-1] #drop last entry so that end date lines up with original data\n",
    "    data4analysis = data4analysis.interp(time=datetimeleap) #interpolate data to new calendar with all leap days\n",
    "    \n",
    "    false_leap_days = []\n",
    "    for i in range(len(data4analysis.time)): #find indices of false leap days\n",
    "        if (data4analysis.time.values[i].month == 2) and (data4analysis.time.values[i].day == 29) and calendar.isleap(data4analysis.time.values[i].year)==False:\n",
    "            false_leap_days.append(i)\n",
    "    data4analysis = data4analysis.drop_isel(time=false_leap_days) #drop data for false leap days\n",
    "    #print('Done with leap year conversion')\n",
    "        \n",
    "    return data4analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a721442b-564a-4d61-8203-294634ef60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smet_convert(data4analysis, altitude, station_name, station_id, out_name):\n",
    "    #function to convert data prepared by data_prep() to .smet filetype required by SNOWPACK\n",
    "    \n",
    "    if os.path.exists('/glade/work/eperkins/SNOWPACK_data/Input/multiple_season/2029_2100_cubic/'+out_name+'.smet'):\n",
    "        val = input('/glade/work/eperkins/SNOWPACK_data/Input/'+out_name+'.smet already exists. Overwrite current file (Yes/No):')\n",
    "        if val == 'No':\n",
    "            print('Skipping '+station_id+', done with execution')\n",
    "        elif val == 'Yes':\n",
    "            #store data in memory to access individual values\n",
    "            #start = time.time()\n",
    "            for var in ['TA', 'RH', 'ILWR', 'VW', 'DW', 'PSUM', 'TSG', 'TSS']:  #don't need to use .compute() on ISWR since its already a numpy array\n",
    "                data4analysis[var] = data4analysis[var].compute()\n",
    "            #print('Data to Memory Time: '+str({time.time() - start})) #print time it takes to store each variable\n",
    "\n",
    "            #create pandas dataframe of variables to easily build lines\n",
    "            #start = time.time()\n",
    "\n",
    "            data_pd = pd.DataFrame({'time': pd.Series(data4analysis.time.values.astype(\"datetime64[ns]\").astype(str)),\n",
    "                               'TA': pd.Series(data4analysis.TA.values.astype(str)),\n",
    "                               'RH': pd.Series(data4analysis.RH.values.astype(str)),\n",
    "                               'TSG': pd.Series(data4analysis.TSG.values.astype(str)),\n",
    "                               'TSS': pd.Series(data4analysis.TSS.values.astype(str)),\n",
    "                               'PSUM': pd.Series(data4analysis.PSUM.values.astype(str)),\n",
    "                               'VW': pd.Series(data4analysis.VW.values.astype(str)),\n",
    "                               'DW': pd.Series(data4analysis.DW.values.astype(str)),\n",
    "                               'ISWR': pd.Series(data4analysis.ISWR.values.astype(str)),\n",
    "                               'ILWR': pd.Series(data4analysis.ILWR.values.astype(str))})\n",
    "            \n",
    "            data_pd['line'] = data_pd.apply(lambda row: row.time+' '+row.TA+' '+row.RH+' '+row.TSG+' '+\n",
    "                                            row.TSS+' '+row.PSUM+' '+row.VW+' '+row.DW+' '+row.ISWR+' '+row.ILWR, axis = 1)\n",
    "            \n",
    "            #data_pd = pd.DataFrame({'time': pd.Series(data4analysis.time.values.astype(\"datetime64[ns]\").astype(str)),\n",
    "            #                   'TA': pd.Series(data4analysis.TA.values.astype(str)),\n",
    "            #                   'RH': pd.Series(data4analysis.RH.values.astype(str)),\n",
    "            #                   'TSG': pd.Series(data4analysis.TSG.values.astype(str)),\n",
    "            #                   'TSS': pd.Series(data4analysis.TSS.values.astype(str)),\n",
    "            #                   'PSUM': pd.Series(data4analysis.PSUM.values.astype(str)),\n",
    "            #                   'VW': pd.Series(data4analysis.VW.values.astype(str)),\n",
    "            #                   'DW': pd.Series(data4analysis.DW.values.astype(str)),\n",
    "            #                   'ILWR': pd.Series(data4analysis.ILWR.values.astype(str)),\n",
    "            #                   'ISWR': pd.Series(data4analysis.ISWR.values.astype(str))})\n",
    "\n",
    "            #data_pd['line'] = data_pd.apply(lambda row: row.time+' '+row.TA+' '+row.RH+' '+row.TSG+' '+\n",
    "            #                                row.TSS+' '+row.PSUM+' '+row.VW+' '+row.DW+' '+row.ILWR+' '+row.ISWR, axis = 1)\n",
    "\n",
    "            #create the header:\n",
    "            data_smet = [\"\" for x in range(10)]\n",
    "            data_smet[0] = 'SMET 1.1 ASCII'\n",
    "            data_smet[1] = '[HEADER]'\n",
    "            data_smet[2] = 'station_id = '+station_id\n",
    "            data_smet[3] = 'station_name = '+station_name\n",
    "            data_smet[4] = 'latitude = '+str(float(data4analysis.lat))\n",
    "            data_smet[5] = 'longitude = '+str(float(data4analysis.lon))\n",
    "            data_smet[6] = 'altitude = '+str(altitude)\n",
    "            data_smet[7] = 'nodata = -999'\n",
    "            data_smet[8] = 'fields = timestamp TA RH TSG TSS PSUM VW DW ISWR ILWR'\n",
    "            data_smet[9] = '[DATA]'\n",
    "\n",
    "            #write to file\n",
    "            pd.Series(data_smet).append(data_pd.line).to_csv('/glade/work/eperkins/SNOWPACK_data/Input/'+out_name+'.smet', index = False, header = False)\n",
    "            \n",
    "            #create initial snow profile-------------------------------------------------------------------------------       \n",
    "            #make empty ascii list\n",
    "            data_initial = [\"\" for x in range(24)]\n",
    "\n",
    "            #create the header:\n",
    "            data_initial[0] = 'SMET 1.1 ASCII'\n",
    "            data_initial[1] = '[HEADER]'\n",
    "            data_initial[2] = 'station_id = '+station_id\n",
    "            data_initial[3] = 'station_name = '+station_name\n",
    "            data_initial[4] = 'latitude = '+str(float(data4analysis.lat))\n",
    "            data_initial[5] = 'longitude = '+str(float(data4analysis.lon))\n",
    "            data_initial[6] = 'altitude = '+str(altitude)\n",
    "            data_initial[7] = 'nodata = -999'\n",
    "            data_initial[8] = 'ProfileDate = '+str(data4analysis.time[0].values.astype(\"datetime64[ns]\"))\n",
    "            data_initial[9] = 'HS_Last = 0.000'    #currently make up last snow height\n",
    "            data_initial[10] = 'SlopeAngle = 0.0' #make up slope angle of zero\n",
    "            data_initial[11] = 'SlopeAzi = 0.0' #make up slope azimuth of zero (pointing north)\n",
    "            data_initial[12] = 'nSoilLayerData = 0' #no soil layers\n",
    "            data_initial[13] = 'nSnowLayerData = 0' # no snow layers to start\n",
    "            data_initial[14] = 'SoilAlbedo = 0.09' #make up soil albedo\n",
    "            data_initial[15] = 'BareSoil_z0 = 0.200' #make up value for now\n",
    "            data_initial[16] = 'CanopyHeight = 0.00' #no canopy\n",
    "            data_initial[17] = 'CanopyLeafAreaIndex = 0.00' #no canopy\n",
    "            data_initial[18] = 'CanopyDirectThroughfall = 1.00' #no canopy\n",
    "            data_initial[19] = 'WindScalingFactor = 1.00' #make up for now\n",
    "            data_initial[20] = 'ErosionLevel = 0' #make up for now\n",
    "            data_initial[21] = 'TimeCountDeltaHS = 0.000000' #irrelevant since don't actually have any initial data\n",
    "            data_initial[22] = 'fields = timestamp Layer_Thick  T  Vol_Frac_I  Vol_Frac_W  Vol_Frac_V  Vol_Frac_S Rho_S Conduc_S HeatCapac_S  rg  rb  dd  sp  mk mass_hoar ne CDot metamo'\n",
    "            data_initial[23] = '[DATA]'\n",
    "\n",
    "\n",
    "            # open file in write mode\n",
    "            with open(r'/glade/work/eperkins/SNOWPACK_data/Input/'+out_name+'_initial.sno', 'w') as fp_initial:\n",
    "                for item in data_initial:\n",
    "                    # write each item on a new line\n",
    "                    fp_initial.write(\"%s\\n\" % item)\n",
    "            \n",
    "            #print('Time to write: '+str({time.time() - start}))\n",
    "        else:\n",
    "            raise ValueError('Invalid response')\n",
    "    else:\n",
    "        #store data in memory to access individual values\n",
    "        #start = time.time()\n",
    "        for var in ['TA', 'RH', 'ILWR', 'VW', 'DW', 'PSUM', 'TSG', 'TSS']:  #don't need to use .compute() on ISWR since its already a numpy array\n",
    "            data4analysis[var] = data4analysis[var].compute()\n",
    "        #print('Data to Memory Time: '+str({time.time() - start})) #print time it takes to store each variable\n",
    "\n",
    "        #create pandas dataframe of variables to easily build lines\n",
    "        start = time.time()\n",
    "\n",
    "        data_pd = pd.DataFrame({'time': pd.Series(data4analysis.time.values.astype(\"datetime64[ns]\").astype(str)),\n",
    "                           'TA': pd.Series(data4analysis.TA.values.astype(str)),\n",
    "                           'RH': pd.Series(data4analysis.RH.values.astype(str)),\n",
    "                           'TSG': pd.Series(data4analysis.TSG.values.astype(str)),\n",
    "                           'TSS': pd.Series(data4analysis.TSS.values.astype(str)),\n",
    "                           'PSUM': pd.Series(data4analysis.PSUM.values.astype(str)),\n",
    "                           'VW': pd.Series(data4analysis.VW.values.astype(str)),\n",
    "                           'DW': pd.Series(data4analysis.DW.values.astype(str)),\n",
    "                           'ISWR': pd.Series(data4analysis.ISWR.values.astype(str)),\n",
    "                           'ILWR': pd.Series(data4analysis.ILWR.values.astype(str))})\n",
    "            \n",
    "        data_pd['line'] = data_pd.apply(lambda row: row.time+' '+row.TA+' '+row.RH+' '+row.TSG+' '+\n",
    "                                        row.TSS+' '+row.PSUM+' '+row.VW+' '+row.DW+' '+row.ISWR+' '+row.ILWR, axis = 1)\n",
    "\n",
    "\n",
    "        #create the header:\n",
    "        data_smet = [\"\" for x in range(10)]\n",
    "        data_smet[0] = 'SMET 1.1 ASCII'\n",
    "        data_smet[1] = '[HEADER]'\n",
    "        data_smet[2] = 'station_id = '+station_id\n",
    "        data_smet[3] = 'station_name = '+station_name\n",
    "        data_smet[4] = 'latitude = '+str(float(data4analysis.lat))\n",
    "        data_smet[5] = 'longitude = '+str(float(data4analysis.lon))\n",
    "        data_smet[6] = 'altitude = '+str(altitude)\n",
    "        data_smet[7] = 'nodata = -999'\n",
    "        data_smet[8] = 'fields = timestamp TA RH TSG TSS PSUM VW DW ISWR ILWR'\n",
    "        data_smet[9] = '[DATA]'\n",
    "\n",
    "        #write to file\n",
    "        pd.Series(data_smet).append(data_pd.line).to_csv('/glade/work/eperkins/SNOWPACK_data/Input/'+out_name+'.smet', index = False, header = False)\n",
    "            \n",
    "        #create initial snow profile-------------------------------------------------------------------------------       \n",
    "        #make empty ascii list\n",
    "        data_initial = [\"\" for x in range(24)]\n",
    "\n",
    "        #create the header:\n",
    "        data_initial[0] = 'SMET 1.1 ASCII'\n",
    "        data_initial[1] = '[HEADER]'\n",
    "        data_initial[2] = 'station_id = '+station_id\n",
    "        data_initial[3] = 'station_name = '+station_name\n",
    "        data_initial[4] = 'latitude = '+str(float(data4analysis.lat))\n",
    "        data_initial[5] = 'longitude = '+str(float(data4analysis.lon))\n",
    "        data_initial[6] = 'altitude = '+str(altitude)\n",
    "        data_initial[7] = 'nodata = -999'\n",
    "        data_initial[8] = 'ProfileDate = '+str(data4analysis.time[0].values.astype(\"datetime64[ns]\"))\n",
    "        data_initial[9] = 'HS_Last = 0.000'    #currently make up last snow height\n",
    "        data_initial[10] = 'SlopeAngle = 0.0' #make up slope angle of zero\n",
    "        data_initial[11] = 'SlopeAzi = 0.0' #make up slope azimuth of zero (pointing north)\n",
    "        data_initial[12] = 'nSoilLayerData = 0' #no soil layers\n",
    "        data_initial[13] = 'nSnowLayerData = 0' # no snow layers to start\n",
    "        data_initial[14] = 'SoilAlbedo = 0.09' #make up soil albedo\n",
    "        data_initial[15] = 'BareSoil_z0 = 0.200' #make up value for now\n",
    "        data_initial[16] = 'CanopyHeight = 0.00' #no canopy\n",
    "        data_initial[17] = 'CanopyLeafAreaIndex = 0.00' #no canopy\n",
    "        data_initial[18] = 'CanopyDirectThroughfall = 1.00' #no canopy\n",
    "        data_initial[19] = 'WindScalingFactor = 1.00' #make up for now\n",
    "        data_initial[20] = 'ErosionLevel = 0' #make up for now\n",
    "        data_initial[21] = 'TimeCountDeltaHS = 0.000000' #irrelevant since don't actually have any initial data\n",
    "        data_initial[22] = 'fields = timestamp Layer_Thick  T  Vol_Frac_I  Vol_Frac_W  Vol_Frac_V  Vol_Frac_S Rho_S Conduc_S HeatCapac_S  rg  rb  dd  sp  mk mass_hoar ne CDot metamo'\n",
    "        data_initial[23] = '[DATA]'\n",
    "\n",
    "        # open file in write mode\n",
    "        with open(r'/glade/work/eperkins/SNOWPACK_data/Input/'+out_name+'_initial.sno', 'w') as fp_initial:\n",
    "            for item in data_initial:\n",
    "                # write each item on a new line\n",
    "                fp_initial.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a78d78-5aad-4fa5-9d69-0c18c50ff3e5",
   "metadata": {},
   "source": [
    "## **Choose points and perform data preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17f306f2-2ec9-4630-83c7-c1750d4de598",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set coordiantes and altitudes for points of interest\n",
    "lat_coords = [66.44,67.38,68.32,69.27,67.38,67.38,68.32,68.32]\n",
    "lon_coords = [26.25,26.25,27.5,28.75,25,27.5,28.75,26.25]\n",
    "\n",
    "#google earth altitudes\n",
    "alt = [100.61, 262.57, 387, 160.07, 211.44, 170.45, 462.5, 285.1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8a9faec-9319-475f-9208-7d7dc1467148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total time for point 1:{783.0815351009369}\n",
      "Total time for point 2:{766.782244682312}\n",
      "Total time for point 3:{732.2177739143372}\n",
      "Total time for point 4:{740.0409495830536}\n",
      "Total time for point 5:{809.2094390392303}\n",
      "Total time for point 6:{789.7881379127502}\n",
      "Total time for point 7:{782.3880794048309}\n",
      "Total time for point 8:{737.0128750801086}\n",
      "\u001b[1mTotal time for run 2:{6270.606032848358}\u001b[0m\n",
      "CPU times: user 1h 3min 13s, sys: 1min 31s, total: 1h 4min 45s\n",
      "Wall time: 1h 44min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#prevent warnings from printing in output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#perform data prep and conversion for all points and all ensemble members\n",
    "for run in range(1,11):\n",
    "    start = time.time()\n",
    "    at_data, rh_data, u_data, v_data, sw_data, lw_data, gt_data, st_data, prcp_data = read_data(run)\n",
    "    run_str = ('00'+str(run))[len('00'+str(run))-3:]\n",
    "    \n",
    "    for pt in range(1,9):\n",
    "        begin = time.time()\n",
    "        data4analysis = data_prep(at_data, rh_data, u_data, v_data, sw_data, lw_data, gt_data, st_data, prcp_data, lat_coords[pt-1], lon_coords[pt-1], 0.1, '2015-08-01', '2100-06-15')\n",
    "        smet_convert(data4analysis, alt[pt-1], 'CESM_Point'+str(pt), 'Point'+str(pt), 'CESM2_LE_Point'+str(pt)+'_2029_2100_LE'+run_str+'_2015')\n",
    "        print('Total time for point '+str(pt)+':'+str({time.time() - begin}))\n",
    "    print('\\033[1mTotal time for run '+str(run)+':'+str({time.time()-start})+'\\033[0m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-analysis3]",
   "language": "python",
   "name": "conda-env-miniconda3-analysis3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
