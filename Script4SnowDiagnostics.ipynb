{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "331341f4-4e47-40a6-8ca9-6606c1c07498",
   "metadata": {},
   "source": [
    "# **Script4SnowDiagnostics**  \n",
    "**Description:** Notebook to read in raw data of interest extracted using Script4ProfileConversion.ipynb and count the number of days/instances with problmatic snow conditions.    \n",
    "**Input Data:** .csv files of raw profile data for each point and ensemble member  \n",
    "**Output Data:** .csv files of counts of days with problmatic snow conditions each season  \n",
    "**Creator:** Emma Perkins  \n",
    "**Date:** November 2023, Updated January 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "59eca589-b17a-41dc-96c4-fba8e790c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import relevant packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import glob\n",
    "import xarray as xr\n",
    "from statsmodels.formula.api import ols\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.api as sm\n",
    "import os\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b9779-6ee3-497d-a253-4f203ed0b326",
   "metadata": {},
   "source": [
    "## **Collect All Data from 1950 - 2100 in Same .csv Files**  \n",
    "**ONLY RUN THIS CELL ONCE, ALREADY RUN ON 11/14/2023**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f1b083-7ea9-4a2d-9a7d-2e57335c3330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path to csv files\n",
    "main_path = '/glade/work/eperkins/SNOWPACK_data/Results/analysis/GE_alts/csv_files/'\n",
    "hist_files = sorted(glob.glob(main_path+'1950_2014/LE*/*.csv'))\n",
    "future_files = sorted(glob.glob(main_path+'2015_2100/LE*/*.csv'))\n",
    "\n",
    "#read in data from all csv files\n",
    "for i in range(80):\n",
    "    LE = 'LE'+str(math.ceil((i+1)/8))\n",
    "    point = hist_files[i].split('/')[-1][0:-4]\n",
    "    pd.concat([pd.read_csv(hist_files[i]),pd.read_csv(future_files[i])]).to_csv(main_path+'1950_2100/'+point+'/cesm2_'+LE+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31027f5-386e-4494-ba1c-156ff2752d32",
   "metadata": {},
   "source": [
    "## **Supporting Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "7edd541b-6a94-4d25-a735-2adffa4bfc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(season_data, depth_cutoff):\n",
    "    #season data: pandas dataframe\n",
    "    #depth cutoff: number for deep day cutoff in cm\n",
    "    def day_counter(unique_dates, duration): \n",
    "        #function to count number of instances of consecutive days that a condition is met\n",
    "        duration_met = 0\n",
    "        consec_days = 0\n",
    "        max_days = 0\n",
    "        for i in range(len(unique_dates)-1):\n",
    "            if unique_dates[i+1] == unique_dates[i] + datetime.timedelta(days=1):\n",
    "                consec_days += 1\n",
    "                if (consec_days >= duration -1) and (i ==  len(range(len(unique_dates)-1)) - 1): #account for the condition that all dates are consecutive\n",
    "                    duration_met += 1\n",
    "            elif consec_days >= duration - 1:\n",
    "                duration_met += 1\n",
    "                consec_days = 0\n",
    "            if consec_days > max_days:\n",
    "                max_days = consec_days\n",
    "        return(duration_met, max_days)\n",
    "    \n",
    "    #count total number of high density days\n",
    "    high_density = season_data[season_data.max_den > 350].day.unique()\n",
    "    hd5, hd_count_max = day_counter(high_density,5)\n",
    "\n",
    "    #count total number of icy days -  days where density is greater than 350 and water content is less than 2%\n",
    "    icy_snowpack = season_data[((season_data.max_den > 350) & (season_data.WC_dmax < 0.1)) | (season_data.max_ice_vol > 70)].day.unique()\n",
    "    ice5, ice_count_max = day_counter(icy_snowpack, 5)\n",
    "\n",
    "    #count length of snow season\n",
    "    season_start = season_data[season_data.depth > 0].reset_index().date[0].dayofyear\n",
    "    season_end = season_data[season_data.depth > 0].reset_index().date[len(season_data[season_data.depth > 0].reset_index()) - 1].dayofyear\n",
    "    season_length = (season_data[season_data.depth > 0].iloc[-1].date - season_data[season_data.depth > 0].iloc[0].date).days\n",
    "\n",
    "    #count number of days with high snowpack\n",
    "    deep_days = season_data[season_data.depth > depth_cutoff].day.unique()\n",
    "    deep5, deep_count_max = day_counter(deep_days, 5)\n",
    "    \n",
    "    return [hd5, hd_count_max, len(high_density), ice5, ice_count_max, len(icy_snowpack), season_start, season_end, season_length, deep5, deep_count_max, len(deep_days)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "e586cbcc-4061-4e3c-bf9d-7dbf3000fe2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#supporting function to append summary stats from single season to dataframe\n",
    "def append_stats_multiyear(depth_cutoff):\n",
    "    ss = data.groupby('season').apply(lambda df: calculate_stats(df, depth_cutoff))\n",
    "    for i in range(len(ss)):\n",
    "        summary_stats.loc[len(summary_stats)] = pd.Series({'season': ss.index[i], 'hd5': ss.iloc[i][0], 'hd_count_max': ss.iloc[i][1],'hd_total': ss.iloc[i][2],\n",
    "                                                           'ice5': ss.iloc[i][3], 'ice_count_max': ss.iloc[i][4], 'ice_total': ss.iloc[i][5],\n",
    "                                                           'season_start': ss.iloc[i][6], 'season_end': ss.iloc[i][7], 'length': ss.iloc[i][8],\n",
    "                                                           'deep5': ss.iloc[i][9], 'deep_count_max': ss.iloc[i][10], 'deep_total': ss.iloc[i][11]})\n",
    "    return summary_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a97b93-01ca-4abd-a9b0-5deff96e2345",
   "metadata": {},
   "source": [
    "## **Calculate snowpack diagnostics for all points**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "a28a156c-554f-40ce-94c8-77a627cdf3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with Point: 1\n",
      "Done with Point: 2\n",
      "Done with Point: 3\n",
      "Done with Point: 4\n",
      "Done with Point: 5\n",
      "Done with Point: 6\n",
      "Done with Point: 7\n",
      "Done with Point: 8\n",
      "CPU times: user 2min 21s, sys: 2.36 s, total: 2min 23s\n",
      "Wall time: 2min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "#load all data\n",
    "main_path = '/glade/work/eperkins/SNOWPACK_data/Results/analysis/GE_alts/'\n",
    "read_data_path = main_path + 'csv_files/1950_2100/'\n",
    "write_data_path = main_path + 'snow_chars/1950_2100/'\n",
    "\n",
    "file_lists = list(range(8))\n",
    "dfs = []\n",
    "depth_cutoff = 50 #adjust to liking, currently 50cm\n",
    "\n",
    "for i in range(8):\n",
    "    file_lists[i] = sorted(glob.glob(read_data_path+'Point'+str(i+1)+'/cesm2_*.csv'))\n",
    "    df = []\n",
    "    for j in range(10):\n",
    "        #read in the data and reformat for easy analysis\n",
    "        data = pd.read_csv(file_lists[i][j]).drop(['Unnamed: 0', 'Unnamed: 0.1'], axis=1) #drop extra index columns\n",
    "        data['date'] = pd.to_datetime(data.date) #convert date from string to datetime\n",
    "        data['day'] = data.date.apply(lambda x: datetime.date(x.year, x.month, x.day)) #add column of day (as opposed to also hour) for counting consecutive days\n",
    "        \n",
    "        #count days with problematic snow conditions\n",
    "        #initialize summary stats dataframe\n",
    "        summary_stats = pd.DataFrame(columns=['hd5', 'hd_count_max', 'hd_total','ice5','ice_count_max','ice_total', 'season_start', 'season_end',\n",
    "                                              'length','deep5','deep_count_max','deep_total','season']) #initializes empty dataframe of summary stats\n",
    "\n",
    "        summary_stats = append_stats_multiyear(depth_cutoff)\n",
    "        for var in ['hd5', 'hd_count_max', 'hd_total','ice5','ice_count_max','ice_total', 'season_start', 'season_end', 'length','deep5','deep_count_max','deep_total','season']:\n",
    "            summary_stats[var] = summary_stats[var].astype(int)\n",
    "        summary_stats.to_csv(write_data_path+'Point'+str(i+1)+'/start_end'+str(j+1)+'.csv')\n",
    "    print('Done with Point: '+str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9820f-aa9b-4b6e-a7f9-6989139baf77",
   "metadata": {},
   "source": [
    "## **Convert diagnostic statistics to NetCDF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "efe12487-2d3e-4903-8679-fb8582289a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load all data\n",
    "main_path = '/glade/work/eperkins/SNOWPACK_data/Results/analysis/GE_alts/snow_chars/1950_2100/'\n",
    "\n",
    "file_lists = list(range(8))\n",
    "dfs = []\n",
    "\n",
    "for i in range(8):\n",
    "    file_lists[i] = sorted(glob.glob(main_path+'Point'+str(i+1)+'/start_end*.csv'))\n",
    "    df = []\n",
    "    for j in range(10):\n",
    "        df.append(pd.read_csv(file_lists[i][j]).drop(['Unnamed: 0'], axis = 1))\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "0794c141-1a45-4e03-a2c3-8c2c172a9ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a numpy array with the same dimensions as the intended NetCDF file\n",
    "var_lists = []\n",
    "\n",
    "for var in ['hd5', 'hd_count_max', 'hd_total', 'ice5', 'ice_count_max', 'ice_total', 'season_start', 'season_end', 'length', 'deep5', 'deep_count_max', 'deep_total']:\n",
    "    master_list = []\n",
    "    for i in range(8):\n",
    "        sub_list = []\n",
    "        for j in range(10):\n",
    "            sub_list.append(dfs[i][j][var].to_numpy())\n",
    "        master_list.append(sub_list)\n",
    "    var_lists.append(np.array(master_list).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "fd47f572-80cc-45d5-9814-77c73918f4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#build an Xarray Dataset using the data from the numpy array above\n",
    "seasons = dfs[0][0].season\n",
    "points = [1,2,3,4,5,6,7,8]\n",
    "runs = [1,2,3,4,5,6,7,8,9,10]\n",
    "\n",
    "full_data = xr.Dataset(\n",
    "    data_vars=dict(\n",
    "        hd5=(['season','run','point'],var_lists[0]),\n",
    "        hd_count_max=(['season','run','point'],var_lists[1]),\n",
    "        hd_total=(['season','run','point'],var_lists[2]),\n",
    "        ice5=(['season','run','point'],var_lists[3]),\n",
    "        ice_count_max=(['season','run','point'],var_lists[4]),\n",
    "        ice_total=(['season','run','point'],var_lists[5]),\n",
    "        season_start=(['season','run','point'],var_lists[6]),\n",
    "        season_end=(['season','run','point'],var_lists[7]),\n",
    "        length=(['season','run','point'],var_lists[8]),\n",
    "        deep5=(['season','run','point'],var_lists[9]),\n",
    "        deep_count_max=(['season','run','point'],var_lists[10]),\n",
    "        deep_total=(['season','run','point'],var_lists[11]),\n",
    "    ),\n",
    "    coords=dict(\n",
    "        season=(['season'], seasons),\n",
    "        run=(['run'], runs),\n",
    "        point=(['point'], points)\n",
    "    )\n",
    ")\n",
    "\n",
    "#save the new dataset as a NetCDF file\n",
    "full_data.to_netcdf('/glade/work/eperkins/SNOWPACK_data/Results/analysis/GE_alts/NetCDF_files/1950_2100_md350_wc01_iv70_start_end.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniconda3-analysis3]",
   "language": "python",
   "name": "conda-env-miniconda3-analysis3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
